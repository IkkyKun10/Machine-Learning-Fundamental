# -*- coding: utf-8 -*-
"""Submission - Time Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g-8WRGIiU2UJpbZrQzv4zTZ7l6blclhX
"""

# import library
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from keras.layers import Dense, LSTM

#import dataset
data_train = pd.read_csv('DailyDelhiClimateTrain.csv')
data_train

#convert datetime
data_train.date = pd.to_datetime(data_train.date)

datatrain_new = data_train.drop(["meantemp", "humidity", "meanpressure"], axis=1)
datatrain_new

date = datatrain_new["date"].values
wind_speed = datatrain_new["wind_speed"].values

#plot wind speed
plt.figure(figsize=(15,5))
plt.grid()
plt.plot(date, wind_speed, color="r")
plt.title('Kecepatan Angin', fontsize=20)

X_train, X_test, y_train, y_test = train_test_split(
    wind_speed, date,
    test_size=0.2,
    random_state=0,
    shuffle=False
)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(X_train, window_size=60, batch_size=100, shuffle_buffer=1000)
test_set = windowed_dataset(X_test, window_size=60, batch_size=100, shuffle_buffer=1000)

ly = tf.keras.layers

model = tf.keras.models.Sequential([
                                    ly.LSTM(64, return_sequences=True),
                                    ly.LSTM(64, return_sequences=True),
                                    ly.Dense(32, activation="relu"),
                                    ly.Dense(16, activation="relu"),
                                    ly.Dense(1),
                                    ly.Lambda(lambda x: x * 400)
])

optimizer = tf.keras.optimizers.SGD(learning_rate=1.000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

#set min var
X_minimum = datatrain_new["wind_speed"].min()
X_minimum

#set max var
X_maximum = datatrain_new["wind_speed"].max()
X_maximum

#set average
X_avg = (X_maximum - X_minimum) * (10 / 100)
X_avg

class myCallBack(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get("mae") < X_avg):
      print("\n MAE of the model < 10% of data scale.\n Stop training...")
      self.model.stop_training = True

callbacks = myCallBack()

num_epoch = 100

history = model.fit(
    train_set,
    epochs=num_epoch,
    validation_data=test_set,
    callbacks=[callbacks]
)

plt.plot(history.history["mae"])
plt.plot(history.history["val_mae"])
plt.title("Model MAE")
plt.ylabel("MAE")
plt.xlabel("Epoch")
plt.legend(["Train", "Test"], loc="upper right")
plt.show()

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend(["Train", "Test"], loc="upper right")
plt.show()